人工智能之梦：从语言模型开始

第一章:序言

首先得说明，按照我个人的观点，从语言模型，尤其是大语言模型作为切入点，尝试理清现代人工智能技术的脉络是一个糟糕的想法。作者本人的求学历程正好伴随着“连接主义AI”的最近一次高潮历程，我亲眼见证了以“卷积神经网络”为代表的AI技术从野蛮生长到逐渐具备完善的数学性和可解释性，也经历了神经网络模型的训练从“盲人摸象”到“经验主义”，再到发展出成体系的指导思想。那是个群雄逐鹿的年代，还没有现在这样成熟好用的TensorFlow,PyTorch，我和我的导师一起一行一行的读完了贾扬清的第一版Caffe框架，开启了我自己的Ai之旅。但在今天这样的时候，再去重走当年的长征路，未免显得有些傲慢与不合时宜，那我们不妨就从大家最感兴趣也是时下最热门的东西--“大语言模型”入手吧。

要想搞清楚什么是大语言模型，首先要搞清楚什么是语言模型，而面对“语言模型”这四个字，我们应该如何着手去了解它呢？思考过后，我决定先抛出一些问题：
我们获取一个语言模型的目的是什么，语言模型与其他模型的区别是什么，这件任务的挑战是什么，为什么语言模型都比较大，而且越来越大？
更基本的，当我们说“模型”这个词的时候，我们想表达的是什么？与之相对的又是什么？语言模型需要什么样的计算，这些计算对算力设备提出了哪些要求？
再往深处，我们能否回答诸如：“为什么chatgpt看起来那么像我们想像中的人工智能”这样的问题？当我们谈论“AI”时，我们又是在谈论什么？
这些问题可能并不够专业，或许也没能涵盖你最好奇的一些问题，不过请相信我，当我们走到最后，你会有你自己那个问题的答案的。

在旅程的开始阶段，我们需要挑选些特定的装备。基于各种考虑，我们不妨一同复习一下一些有关与深度学习的知识，以免频繁的遇到需要解释的名词。
1. 训练。深度学习（事实上机器学习类似）可以看作是在“寻找一种函数”，这种函数可能非常复杂，以至于难以通过直接给出表达式的方法获得，也无法通过简单的尝试求解微分方程组的方式得到。但是人们可以相对容易的搜集到大量关于这个目标函数的输入和输出的信息。因此我们并不是没有这个目标函数，而是这个目标函数以一种不太方便使用的形式存在于一个黑盒中（例如这个函数是人类的一种能力，是敌军的通信密码本，或者自然界的一种难以模拟的现象等等）。出于将一切自动化的这种愿望，我们尝试用某种方法“破解”这个黑盒子，正如在战争时期破解任何密码一样，这项工作需要大量的“密文”和与之对应的“明文”作为研究素材，在我们的场景中，如果破译工作是根据“密文-明文对”来进行，则称为有监督训练，如果我们没有任何一条明文，仅有大量的密文并尝试进行破解，则称为无监督训练（或无监督学习），而折衷的情况则称为半监督训练。特别说明，在半监督状态下，数据通常不是“密文-明文对”形式存在的，而是密文和与明文相关的某种其他东西，例如对敌军的侦察情报。特别的，在我们的场景中，把“密文”称之为语料。
对于“只有密文而没有明文”的这种情况，会让一部分不了解密码学和机器学习的朋友感到疑惑：这看起来是完全不可实现的任务。实际上则不然，自然语言中存在着许多人类肉体凡胎难以捕捉到的信息，例如对大量的英文语料进行统计，会发现不同字母出现的频率不同，这种特性不会被替换密码所破坏，因此只要收集足够的语料，就能在完全没有明文的情况下破解替换密码（E的频率是最高的，笔者对此深有体会，因为笔者自己曾经用坏过的所有键盘中，左shift键和E键是磨损最高的）。你可能已经发现了，在我们的LLM场景下，明文对应着问题，而密文对应着答案，而收集足够多的这种对应关系似乎是一件难以完成的任务，因此我们的任务就很类似于这种“在没有明文对应的情况下尝试破解敌人的密码”。

2.损失函数。你几乎可以在任何深度学习相关的文章中看到这个词，我们可以简单的把他理解为“训练目标就是要最小化这个函数”。如果你对反向传播之类的概念稍有了解，你应该可以说出这样的话：损失函数的值是训练动力的来源。在我们的例子中，你需要时刻记住一件事：损失函数往往最能说明一个模型到底具有什么能力，或者说他是被训练来干什么的。例如在CV任务中，当你看到一个模型使用了回归性损失函数，你就可以猜测他很可能具有“某个东西在某个位置”这样的能力。

3. 涌现性。这个词被应用来形容LLM的某种能力。让我先尝试解释他原本的含义：简单的来说就是“量变引发质变”，但我想请大家注意的是，通常量变引发质变会被用在这样的一个场景下：父母教导孩子，刷一万遍做错的题就再也不会做错了，就像你几乎不可能算错1+1那样。请注意这个不是“涌现”，涌现指的是大量被简单规则支配的客体的集合能够表现出超越支配个体的简单规则的性质的一种现象。比如蚂蚁和蜜蜂的个体行为模式极其简单，但是蚁群和蜂群能够表现出“真社会性”。再比如游戏我的世界中仅仅规定了一些基本的物理规则，但是有人能利用这些符合真实物理法则的小方块在游戏里造出芯片和计算机。在我们的例子中，涌现性指的是：我们用极其简单的约束进行模型的训练，但是模型表现出了一些复杂特性，这些特性并不包含在我们明确提供的约束中。例如我们

4. embedding。这是一个稍微有点复杂的概念，但却是几乎一切现代语言模型的基石。尝试思考这样的一个问题：我们现在有一个英文语料库，英文是由单词组成句子的，英文单词的总量可能是很大的（我估计远大于一百万，因为在我上初中的时候看过一条新闻，第一百万个单词web2.0诞生了），我们的模型应当有能力认识并输出语料库中的任何一个单词，那么可以想象把每个单词按照字典序列编一个从1到一百万的唯一数字，再把原来的句子替换成这个数字，这样看起来更像是一个能被数学系统处理的东西了。
但是且慢，这里存在要给显而易见的问题：你不能让这个编号直接参与计算，因为这意味着字典序靠后的单词带有一个超级大的数字，这可能会带来两个棘手的问题：首先，我们的模型可能非常复杂，输入数据要经过很多次运算，那么对于一百万这样的数字，就很容易溢出；其次，数值的大小很可能会导致模型更青睐于输出那些字典序靠后/靠前（这却决于损失函数的构造）的单词，我们把这种现象一般称为“退化”，也就是模型在损失函数的约束下找到了一种能最小化损失值的捷径，导致损失函数没能像我们想像的那样引导模型走向正确的方向。对于问题1，CV任务中常用的诸如归一化之类的方式是有害的，因为这可能会快速的破坏掉语料原本的结构信息，而对于问题2，看似能够通过设计更好的损失函数来实现，实则不然，过于复杂的损失函数往往并不是一个好的主意，况且这也更难设计。
出于“平等的表示字典中的每一个单词”和“避免出现大小悬殊的数值”这两个目的，我们可能会想到使用“onehot”编码，想象一个语料库中含有K个单词，其中第k个单词就用一个只有第k位为1，其他K-1位都为0的向量表示。这个方法就能完美的解决上述两个问题，只不过他会浪费大量的存储空间，尤其是K通常都特别大，随着语料库的扩展，单单是这种向量构成的句子，就会很快超过任何一种已知的GPU的显存的大小。
因此我们希望能够找到一种方法，把超级长的这种onehot编码组成的矩阵压缩成一个固定大小的向量，并且不损失其中携带的信息。我们并不像在这里展开embedding的具体计算方法，读者在这里只需要知道，我们成功的找到了这么一种方法，能够把那个很夸张的K列的矩阵压缩到很小，特别的，有的时候这个操作就叫embedding，有的时候这种操作的结果被叫做embedding，有的时候这种操作所需的那个“查找表”被称为embedding，希望读者在看其他论文的时候不会因此而困惑。相应的，因为查找表的存在，因此有的时候会把编码前的信息叫做“index”，有的时候会把查找表本身叫做index，有的时候也会把查出来的东西叫做index，我认为纠结这种东西是没有任何意义的，搞清楚embedding的作用就是方便我们把长成千奇百怪的语料编码成一个可接受的大小的向量并输入到他后面的数学系统里参与计算就可以了。

5. 没有免费的午餐。换句话说就是“凡事都有代价”。你可能会奇怪这种话题为什么会出现在一篇关于LLM的文章中，其实这是一个在几乎所有的机器学习/深度学习任务中都有意义的话题。例如刚刚关于embedding的讨论中，如果你仔细思考实际上没有办法完全避免任何“超级大的数据结构”的存储，即使我们用了很多的技巧，那个“查找表”也不可能变的很小，毕竟你不能越过信息极限。因此在接下来的故事中，你可以多思考这个问题：“代价是什么”。顺带一提，在我们真实世界中存在一个非常神奇的定律叫做“贝肯斯坦极限”，它大致上可以理解为对于任何空间能存储的信息的上限与一个与此空间体积相同的球壳的表面积成正比，或许未来的某一天这将会成为制约人类把计算机和存储器越做越小的障碍。

6. 数据的分布。在机器学习中，你永远会面对一个问题：无论你的数据集有多大多全面，它永远是真实世界的过去时，永远会有你的模型从来没见过的东西出现。对于这个问题，你可以期待模型拥有“随机应变”的能力，但相信我这是一件难以捉摸的事情。你当然也可以不停的补充数据进行微调，几乎所有的商用模型都在不停的干这件事，但更重要的是理解“同分布”的概念---你永远可以相信大数定律。我们提出两个基本假设：1.对于任何有意义的任务，他可能出现的输入数据是符合某种分布的；2.我们能够收集一个有限数据集，使得该数据集与真实数据同分布。在这样的假设下我们可以相信：我们的模型一旦在此数据集上成功训练，能够在真实数据上获得接近训练效果的使用效果。那么什么是“同分布”呢？一个简单替换密码和他的明文是同分布的，同一个城市采集的交通实况录像是同分布的，而在非洲草原拍摄的野生动物照片和在热带雨林拍摄的野生动物照片可能就不是同分布的。实际上学界已经有很多颇具数学性的方法可以进行判定，如果你熟悉机器学习，应该会感觉这种判定类似于一种叫“聚类”的任务。数据集的分布状态对模型性能的影响是非常大的，如果你尝试训练自己的模型，一定要注意这个问题。

7. 一切都是信息，一切关于信息。了解相对论和近现代物理学的读者应该对光速是本宇宙的速度上限这件事有所印象。但如果你了解的更多一点，就会知道事实上存在很多“超光速”现象，但这些现象有一个共同点：无法传递任何信息。进入现代，越来越多的现象和研究暗示着一个令人深思的可能性：相比于物质和能量，信息才是第一位的。
对于深度学习系统来说，我们需要时刻把握住一件事：信息不能凭空产生，他只能从一种形式转换为另一种形式，信息一旦灭失和耗散就无法还原。比如对一张图片做pooling，每个四像素邻域取一个最大值保留，这张图片就会缩小，其中max这个操作就会导致信息灭失，即使我们在pooling时保存那个最大值的原始位置，也不可能还原灭失掉的信息，此时复原原图是不可能的。但请注意，信息灭失不一定时坏事，深度学习系统总归时要为人类服务的，例如一个分类模型可以识别一张照片里的小动物时猫还是狗，输入了一张图片，但是只输出了1bit的信息，显然有大量的信息灭失掉了。控制信息的转换和压缩是设计一个深度学习系统的要点。

好了，到了这里，我们应该对于LLM所会面对的一些基本的问题和概念有了一个认识，是时候出发了。

第二章：数学很重要
本文致力于给从未涉足过深度学习和机器学习的朋友一个入门级的科普。虽然数学公式的数量和读者的兴趣成负相关，但是Ai是一件复杂的事情，我希望读者们能尽可能的理解“如果一个人说他在做深度学习，那么他每天都在想什么”这样的一个问题。这里的症结在于，如果清楚了其中的数学把戏，你将会更加容易理解那些自称为“数据科学家”和“高性能计算工程师”的人是如何判断一个模型的性能瓶颈和它将会面临哪些挑战的。相信我，这样能让你在和朋友谈论深度学习时显得更加专业，那么我也算不白写这么长。

我决定使用一个叫做“DLRM”的模型来做为例子。这个模型是一个推荐系统模型，他缺少语言模型常用的transformer和循环结构，但它具有embedding和interaction等结构，而且足够用于展示相关的数学计算。请大家注意，虽然这个模型是一个看起来非常简单的模型，但是他依然会面临很多可怕的挑战，在这一点上，LLM也是类似的。你可能会看过一些神经网络的图示，有一些模型看起来非常复杂和巨大，但是他其中含有的计算却并不那么困难；而另一些模型的结构看起来紧凑，但却具有令人无法相信的巧思和困难：不要以貌取模型。

